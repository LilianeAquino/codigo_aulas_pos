{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "embeddings pre-treinados.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zntsLvwH72SO"
      },
      "source": [
        "# baseado no tutorial: https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras import Input, Model\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, GlobalMaxPooling1D\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import codecs\n",
        "import gensim\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-s0sI7z8clS"
      },
      "source": [
        "# va na aula \n",
        "# faça o upload do arquivo FakeCorpus.zip e execute\n",
        "!unzip 'FakeCorpus.zip' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JeFHgOB72SW"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 200\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbdSiVsx72Sb"
      },
      "source": [
        "# Carregando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9GJyZKJ72Sb"
      },
      "source": [
        "def load_data(data_dir, class_label):\n",
        "\n",
        "    file_names = []\n",
        "    for (dirpath, dirnames, filenames) in os.walk(data_dir):\n",
        "        for f in filenames:\n",
        "            if f.endswith(\".txt\"):\n",
        "                file_names.append(os.path.join(dirpath, f))\n",
        "\n",
        "    txt_ = []\n",
        "    y = []\n",
        "    for f in file_names: \n",
        "        txt_.append(codecs.open(f,'r',encoding='iso8859-1').read())\n",
        "        y.append(class_label)\n",
        "\n",
        "    return txt_, y\n",
        "\n",
        "fake_data, y_fake = load_data('Fake.Br Corpus/full_texts/fake/', 0)    \n",
        "true_data, y_true = load_data('Fake.Br Corpus/full_texts/true/', 1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_obep672Sf"
      },
      "source": [
        "data_train, data_test, y_train, y_test = train_test_split(fake_data + true_data, y_fake + y_true,test_size=0.15, random_state=1447)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE212u4172Si"
      },
      "source": [
        "# Carregando o modelo e processando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sY72MMg72S0"
      },
      "source": [
        "# aqui vamos carregar o modelo, que usamos na aula 03 e em outras aulas, usando o gensim\n",
        "embedding_model = gensim.models.KeyedVectors.load_word2vec_format(\"vectors-wikipt.bin\", binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYXSlVIC72Tp"
      },
      "source": [
        "# primeiro eu tenho que especificar um tokenizador com o vocabulario \n",
        "# do modelo pré-treinado. Daí ele vai saber como tokenizar um texto novo recebido.\n",
        "# Após isso, eu preciso pegar a matriz de embeddings deste modelo\n",
        "# Com essa matriz, eu já inicializo minha camada de Embeddings do Keras e ela \n",
        "# já vai estar inicializada com bons valores para representar minha palavras.\n",
        "# Por isso o resultado, em geral, com este tipo de estratégia, costuma ser melhor \n",
        "# do que depender apenas do treino da camada Embddings do keras\n",
        "def get_tokenizer(embedding_model):\n",
        "\n",
        "    # estou pegando todas as minhas palavras em um formato de lista\n",
        "    all_words = [ w for (w, v) in embedding_model.vocab.items()]\n",
        "\n",
        "    # mapeando minhas palavras do modelo pré-treinado para um índice\n",
        "    # inteiro. Esse índice que representará minha palavra w\n",
        "    vocabulary = {}\n",
        "    for idx, w in enumerate(all_words):\n",
        "        vocabulary[w] = idx + 1\n",
        "\n",
        "    # Inicializando meu tokenizador com o vocabulário já existente\n",
        "    # no meu modelo pré-treinado\n",
        "    tokenizer = Tokenizer(len(vocabulary))\n",
        "    # além disso, eu preciso especificar o mapeamento palavra -> índice\n",
        "    # Assim o keras vai saber localizar a palavra na matriz\n",
        "    # Por exemplo, palavra \"casa\" é índice 10. Então o keras vai na linha 10\n",
        "    # da matriz e pega a representação daquela palavra\n",
        "    # O tokenizador que faz essa \"ponte\"\n",
        "    tokenizer.word_index = vocabulary\n",
        "\n",
        "    # definindo uma matriz de pesos com tamanho (numero_palavras + 1) x tamanho do embedding\n",
        "    # inicialmente essa matriz estará toda com zeros\n",
        "    weight_matrix = np.zeros((len(all_words) + 1, embedding_model.vector_size))\n",
        "    # percorrer as palavras e armazená-las na matriz. Fazendo o mapeamento \n",
        "    # indice (usado no tokenizador) -> vetor embedding no modelo pré-treinado\n",
        "    for i in range(len(all_words)):\n",
        "        weight_matrix[i + 1] = embedding_model[all_words[i]]\n",
        "    \n",
        "    return tokenizer, weight_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccl2qN6B72Ts"
      },
      "source": [
        "# Construir e treinar o nosso modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS9zk_pi72Tt"
      },
      "source": [
        "tokenizer, embedding_matrix = get_tokenizer(embedding_model)\n",
        "# a vetorizacao dos dados, mas feito de outras forma, i.e., usando o tokenizador\n",
        "X_train = tokenizer.texts_to_sequences(data_train) \n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(data_test) \n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNfjKgYs72T0"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "\n",
        "# inicializando a nossa camada Embedding com a matriz de pesos e outras informações\n",
        "# argumento 1: dimensão da entrada, que é o tamanho do nosso vocabulário + 1\n",
        "# argumento 2: tamanho da \"saída\" (que será o nosso vetor embedding)\n",
        "# weights: fizando a matriz de pesos\n",
        "# input_length: tamanho maximo da sequencia que entrará na rede\n",
        "# trainable: False evita que os pesos dessa camada sejam atualizados, como já inicializamos com uma matriz de pesos\n",
        "# que já foi treinada, então não precisamos modificá-la\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ZSlgOL72T2"
      },
      "source": [
        "# construindo a arquitetura do nosso modelo\n",
        "\n",
        "sequence_input = Input(shape=(None,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "#x = Conv1D(128, 5, activation='relu')(x)\n",
        "#x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "preds = Dense(1, activation='softmax')(x)\n",
        "\n",
        "#learning_rate = 0.00005\n",
        "learning_rate=0.001\n",
        "#learning_rate = 0.005\n",
        "#learning_rate=0.01\n",
        "#learning_rate=0.05\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "#opt = keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUnvQ_Rb72T_"
      },
      "source": [
        "model.fit(X_train, np.array(y_train), epochs=3, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH3KsEHv72UC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}